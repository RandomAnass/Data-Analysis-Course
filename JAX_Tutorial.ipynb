{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RandomAnass/Data-Analysis-Course/blob/main/JAX_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12TM5qL4i3RE",
        "outputId": "6ba219a8-f1ca-44e9-ec8b-a09c18d02d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRmDy8DXi71S",
        "outputId": "8c3d7e7e-9ffb-40df-a2f5-600d82c65f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade jax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Collecting jax\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.7.1,>=0.7.1 (from jax)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.1)\n",
            "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.3\n",
            "    Uninstalling jaxlib-0.5.3:\n",
            "      Successfully uninstalled jaxlib-0.5.3\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.3\n",
            "    Uninstalling jax-0.5.3:\n",
            "      Successfully uninstalled jax-0.5.3\n",
            "Successfully installed jax-0.7.1 jaxlib-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y jax_cuda12_plugin"
      ],
      "metadata": {
        "id": "z4vb8UbU6pmi",
        "outputId": "5b129918-678b-45e1-9fdf-4271b9d1fe22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax-cuda12-plugin 0.5.3\n",
            "Uninstalling jax-cuda12-plugin-0.5.3:\n",
            "  Successfully uninstalled jax-cuda12-plugin-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpRORJ6tkelE"
      },
      "source": [
        "# JAX 1. Numpy Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ewm5mgFi80g",
        "outputId": "66437844-2c1a-4ae4-9855-801674ea98eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.ones((5000, 5000))\n",
        "y = np.arange(5000)\n",
        "\n",
        "%timeit z = np.sin(x) + np.cos(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "497 ms ± 199 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsjQwMS5jL9K",
        "outputId": "c703688f-ca79-4fac-b126-f74051a76ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import jax.numpy as jnp\n",
        "x = jnp.ones((5000, 5000))\n",
        "y = jnp.arange(5000)\n",
        "\n",
        "%timeit z = jnp.sin(x) + jnp.cos(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 11.54 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "94.4 µs ± 135 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s74k_3ekx5r"
      },
      "source": [
        "# JAX 2. JIT Compiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhD5QzxNjTQo"
      },
      "source": [
        "from jax import jit\n",
        "import tensorflow as tf\n",
        "\n",
        "def fn(x, y):\n",
        "  z = np.sin(x)\n",
        "  w = np.cos(y)\n",
        "  return z + w\n",
        "\n",
        "@jit\n",
        "def fn_jit(x, y):\n",
        "  z = jnp.sin(x)\n",
        "  w = jnp.cos(y)\n",
        "  return z + w\n",
        "\n",
        "@tf.function\n",
        "def fn_tf2(x, y):\n",
        "  z = tf.sin(x)\n",
        "  w = tf.cos(y)\n",
        "  return z + w"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl1PIhYsnV2q",
        "outputId": "7ae81dd3-4ae8-47ef-f60f-d2253c1bdc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.ones((5000, 5000))\n",
        "y = np.ones((5000, 5000))\n",
        "%timeit fn(x, y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "644 ms ± 12.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ogfbuO_nTaY",
        "outputId": "5c0b630e-63b6-48ff-b37e-aa88e69bc733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jx = jnp.ones((5000, 5000))\n",
        "jy = jnp.ones((5000, 5000))\n",
        "%timeit fn_jit(jx, jy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avtiy3VPncSS",
        "outputId": "f219902a-3def-49e5-ead9-043cd7071092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tx = tf.ones((5000, 5000))\n",
        "ty = tf.ones((5000, 5000))\n",
        "%timeit fn_tf2(tx, ty)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.83 ms ± 3.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzodSnzgs1Eu"
      },
      "source": [
        "# JAX 3. grad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLyN4sF-ookp"
      },
      "source": [
        "from jax import grad\n",
        "\n",
        "@jit\n",
        "def simple_fun(x):\n",
        "  return jnp.sin(x) / x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPQ_q3J_qo6e"
      },
      "source": [
        "grad_simple_fun = grad(simple_fun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmx864Wrqqkn",
        "outputId": "0099c3f6-5242-4825-f67c-3d3108e812dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%timeit grad_simple_fun(1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 loops, best of 3: 1.22 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NN2Rqr0r-zE",
        "outputId": "0143afee-155e-46e9-c11c-48e1117cb8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "x_range = jnp.arange(10, dtype=jnp.float32)\n",
        "[grad_simple_fun(xi) for xi in x_range]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DeviceArray(nan, dtype=float32),\n",
              " DeviceArray(-0.30116874, dtype=float32),\n",
              " DeviceArray(-0.43539774, dtype=float32),\n",
              " DeviceArray(-0.3456775, dtype=float32),\n",
              " DeviceArray(-0.11611074, dtype=float32),\n",
              " DeviceArray(0.09508941, dtype=float32),\n",
              " DeviceArray(0.16778992, dtype=float32),\n",
              " DeviceArray(0.09429243, dtype=float32),\n",
              " DeviceArray(-0.03364623, dtype=float32),\n",
              " DeviceArray(-0.10632458, dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcZCALcmqrlk"
      },
      "source": [
        "grad_grad_simple_fun = grad(grad(simple_fun))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8kZ-K-Erkfl",
        "outputId": "56669dd6-44dd-48ff-dd2d-71d61342cb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%timeit grad_grad_simple_fun(1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 93.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 3.19 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNaREYVErlkH",
        "outputId": "a19b3fd4-f856-49c1-ef7c-dbe7d8ec2307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grad_grad_simple_fun(1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-0.23913354, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmyVUJrAsGBZ",
        "outputId": "dd01de27-f132-4921-d390-585c75668445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "x_range = jnp.arange(10, dtype=jnp.float32)\n",
        "[grad_grad_simple_fun(xi) for xi in x_range]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DeviceArray(nan, dtype=float32),\n",
              " DeviceArray(-0.23913354, dtype=float32),\n",
              " DeviceArray(-0.01925094, dtype=float32),\n",
              " DeviceArray(0.18341166, dtype=float32),\n",
              " DeviceArray(0.247256, dtype=float32),\n",
              " DeviceArray(0.1537491, dtype=float32),\n",
              " DeviceArray(-0.00936072, dtype=float32),\n",
              " DeviceArray(-0.12079593, dtype=float32),\n",
              " DeviceArray(-0.11525822, dtype=float32),\n",
              " DeviceArray(-0.02216326, dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21PcHaMFrmnN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n"
      ],
      "metadata": {
        "id": "tU_ojz4j51XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hands-on coverage of: JAX basics (jit/grad/vmap), pytrees, PRNG best practices; autodiff theory & APIs (jvp/vjp, jacfwd/jacrev, custom rules); control flow; LAX primitives; performance (tracing, static args, donation, remat, timing); vectorization & parallelism (vmap, pmap, intro to pjit & sharding); numerics & mixed precision; end-to-end models (MLP, CNN, RNN with scan, compact Transformer block); optimization with Optax; saving/loading; debugging/profiling; interop"
      ],
      "metadata": {
        "id": "_bpg1eGG54A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y jax_cuda12_plugin\n",
        "!pip install -U \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "#!pip uninstall jaxlib -y\n",
        "#!pip install \"jax[cuda12]==0.7.1\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ],
      "metadata": {
        "id": "FF_X8qQs6y2Z",
        "outputId": "0f913731-ce62-4a58-9b13-2a58e834035a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.12/dist-packages (0.7.1)\n",
            "Requirement already satisfied: jaxlib<=0.7.1,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.7.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (1.16.1)\n",
            "Collecting jax-cuda12-plugin<=0.7.1,>=0.7.1 (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax-cuda12-pjrt==0.7.1 (from jax-cuda12-plugin<=0.7.1,>=0.7.1->jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.7.1-py3-none-manylinux_2_27_x86_64.whl.metadata (579 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.6.80)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.8 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12>=12.1.55 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Collecting nvidia-nvshmem-cu12>=3.2.5 (from jax-cuda12-plugin[with-cuda]<=0.7.1,>=0.7.1; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.24-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Downloading jax_cuda12_plugin-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.7.1-py3-none-manylinux_2_27_x86_64.whl (123.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.24-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, nvidia-nvshmem-cu12, nvidia-cuda-nvcc-cu12, jax-cuda12-plugin\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.5.3\n",
            "    Uninstalling jax-cuda12-pjrt-0.5.3:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.5.3\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "Successfully installed jax-cuda12-pjrt-0.7.1 jax-cuda12-plugin-0.7.1 nvidia-cuda-nvcc-cu12-12.9.86 nvidia-nvshmem-cu12-3.3.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jax_plugins",
                  "nvidia"
                ]
              },
              "id": "421005daee334fa29427df2e2aed8cf8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U optax flax orbax-checkpoint matplotlib"
      ],
      "metadata": {
        "id": "f_J5Q8cd57Bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3054ddcb-a5ba-4ea1-d4db-b5d7133b93fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.10.6)\n",
            "Collecting flax\n",
            "  Downloading flax-0.11.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (0.11.23)\n",
            "Collecting orbax-checkpoint\n",
            "  Downloading orbax_checkpoint-0.11.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.12/dist-packages (from optax) (0.7.1)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.12/dist-packages (from optax) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint) (3.20.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax) (1.16.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Downloading flax-0.11.2-py3-none-any.whl (458 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.1/458.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_checkpoint-0.11.24-py3-none-any.whl (529 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.3/529.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib, orbax-checkpoint, flax\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.11.23\n",
            "    Uninstalling orbax-checkpoint-0.11.23:\n",
            "      Successfully uninstalled orbax-checkpoint-0.11.23\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.10.6\n",
            "    Uninstalling flax-0.10.6:\n",
            "      Successfully uninstalled flax-0.10.6\n",
            "Successfully installed flax-0.11.2 matplotlib-3.10.6 orbax-checkpoint-0.11.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "fe30ae8cedb247d78f2ae4a10e1d6191"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, math, time, functools, itertools, pickle\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Tuple, Dict\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, value_and_grad, random, lax, vmap, jacrev, jacfwd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L6hg0Jc-69ay"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('JAX version:', jax.__version__)\n",
        "print('Backend:', jax.default_backend())\n",
        "print('Devices:', jax.devices())"
      ],
      "metadata": {
        "id": "4STcMQkh7BNh",
        "outputId": "9f87d7e4-f32c-49d2-cecf-af2b90b7de31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.7.1\n",
            "Backend: gpu\n",
            "Devices: [CudaDevice(id=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "aVxFzBH_7D5A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX basics"
      ],
      "metadata": {
        "id": "25JqRHGV_eCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(arrays, no in-place updates, jit, grad, vmap)\n",
        "Key ideas: functional transforms (jit, grad, vmap) act on functions; no in-place mutation (use .at[...]); first jit call compiles; timings should block with .block_until_ready()."
      ],
      "metadata": {
        "id": "xdVUTUlL_g9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.ones((5, 5))\n",
        "y = jnp.arange(25).reshape(5,5)"
      ],
      "metadata": {
        "id": "uk1lMamxAOJA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)\n",
        "print(x+y)"
      ],
      "metadata": {
        "id": "NhWLd3GZAX4E",
        "outputId": "034d01d2-6f04-4c8c-d0ec-953b7fd29239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]]\n",
            "[[ 1.  2.  3.  4.  5.]\n",
            " [ 6.  7.  8.  9. 10.]\n",
            " [11. 12. 13. 14. 15.]\n",
            " [16. 17. 18. 19. 20.]\n",
            " [21. 22. 23. 24. 25.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrays & broadcasting\n",
        "x = jnp.arange(6).reshape(2, 3)\n",
        "y = jnp.ones((3,))\n",
        "print('x:\\n', x)\n",
        "print('x + y:\\n', x + y)  # broadcasting\n",
        "\n",
        "# No in-place mutation -> use .at\n",
        "z = x.at[:, 1].set(42)\n",
        "print('z (col 1 set to 42):\\n', z)\n",
        "\n",
        "# JIT compilation: first call compiles, subsequent calls are fast\n",
        "@jit\n",
        "def poly(a, b, x):\n",
        "    return a * x**2 + b * x + 1.0\n",
        "\n",
        "xv = jnp.linspace(-3, 3, 100_000)\n",
        "a, b = 2.0, -1.0\n",
        "\n",
        "t0 = time.time()\n",
        "out = poly(a, b, xv).block_until_ready()  # compile + run\n",
        "t1 = time.time()\n",
        "out2 = poly(a, b, xv).block_until_ready() # run-only\n",
        "t2 = time.time()\n",
        "print(f'First call (compile+run): {(t1 - t0)*1e3:.2f} ms')\n",
        "print(f'Second call (run only):   {(t2 - t1)*1e3:.2f} ms')\n",
        "\n",
        "# Autodiff with grad\n",
        "def loss_fn(w, x, y):\n",
        "    pred = w[0] * x + w[1]\n",
        "    return jnp.mean((pred - y)**2)\n",
        "\n",
        "w = jnp.array([2.0, -1.0])\n",
        "x = jnp.linspace(-1, 1, 1000)\n",
        "y = 3.0 * x + 0.5  # true slope=3, intercept=0.5\n",
        "\n",
        "dloss_dw = grad(loss_fn)(w, x, y)\n",
        "print('Gradients:', dloss_dw)\n",
        "\n",
        "# vmap to batch without Python loops\n",
        "def square_plus_one(t):\n",
        "    return t*t + 1\n",
        "\n",
        "batched_square = vmap(square_plus_one)  # vectorize over leading axis\n",
        "print('vmap:', batched_square(jnp.arange(5.)))\n"
      ],
      "metadata": {
        "id": "gXROBfe2BfaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Pytrees (structuring parameters)"
      ],
      "metadata": {
        "id": "63Ql7sdvScDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytree = nested structure of leaves (arrays/scalars). Many JAX APIs accept pytrees directly."
      ],
      "metadata": {
        "id": "KoOVCXzASfl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import tree_util\n",
        "\n",
        "params = {'W': jnp.ones((3, 3)), 'b': jnp.zeros((3,))}\n",
        "print('Leaves:', tree_util.tree_leaves(params))\n",
        "\n",
        "# Map over leaves\n",
        "scaled = jax.tree_map(lambda x: 0.5 * x, params)\n",
        "print('Scaled b:', scaled['b'])\n",
        "\n",
        "# dataclass as pytree\n",
        "@dataclass\n",
        "class MLPParams:\n",
        "    W1: jnp.ndarray\n",
        "    b1: jnp.ndarray\n",
        "    W2: jnp.ndarray\n",
        "    b2: jnp.ndarray\n",
        "\n",
        "p = MLPParams(jnp.ones((4, 8)), jnp.zeros((8,)), jnp.ones((8, 2)), jnp.zeros((2,)))\n",
        "print('Num leaves:', len(tree_util.tree_leaves(p)))\n"
      ],
      "metadata": {
        "id": "VJLvikJLSeP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) PRNG best practices (functional randomness)"
      ],
      "metadata": {
        "id": "BinhSbPHSgt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split keys; treat keys as inputs/outputs; use fold_in(step) for reproducible per-step seeding."
      ],
      "metadata": {
        "id": "-GN-qiAASjc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.PRNGKey(42)\n",
        "key, k1, k2 = random.split(key, 3)\n",
        "print('Normal:', random.normal(k1, (3,)))\n",
        "print('Uniform:', random.uniform(k2, (3,)))\n",
        "\n",
        "# fold_in for deterministic per-step randomness\n",
        "base = random.PRNGKey(123)\n",
        "for step in range(3):\n",
        "    k = random.fold_in(base, step)\n",
        "    print('step', step, '->', random.normal(k, ()).item())\n"
      ],
      "metadata": {
        "id": "BHkgZgm3SkBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tCX8HDuSk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Autodiff theory + APIs (jvp/vjp, jacfwd/jacrev, custom_vjp/jvp)"
      ],
      "metadata": {
        "id": "bbdRP1cgSlK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward-mode AD (jvp, jacfwd) is efficient when input dim ≤ output dim.\n",
        "\n",
        "Reverse-mode AD (grad, vjp, jacrev) is efficient when output dim ≤ input dim.\n",
        "\n",
        "You can nest them for higher-order derivatives.\n",
        "\n",
        "Custom rules (custom_vjp / custom_jvp) = numerical stability or non-standard ops."
      ],
      "metadata": {
        "id": "iqlwnw0eSngW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jvp/vjp example\n",
        "def f(u):\n",
        "    return jnp.array([u[0]*jnp.sin(u[1]), u[0]*jnp.cos(u[1])])  # R^2 -> R^2\n",
        "\n",
        "u0 = jnp.array([2.0, 0.3])\n",
        "tangent = jnp.array([1.0, -0.2])\n",
        "\n",
        "y, jvp_val = jax.jvp(f, (u0,), (tangent,))\n",
        "print('f(u0)=', y)\n",
        "print('JVP(f; u0, tangent)=', jvp_val)\n",
        "\n",
        "y, pullback = jax.vjp(f, u0)\n",
        "bar = jnp.array([1.0, 0.0])  # seed cotangent\n",
        "vjp_val = pullback(bar)[0]\n",
        "print('VJP(f; u0, bar)=', vjp_val)\n",
        "\n",
        "# Jacobians\n",
        "def g(u):\n",
        "    return jnp.array([u[0]**2 + u[1], jnp.sin(u[0]*u[1])])\n",
        "\n",
        "J_fwd = jacfwd(g)(u0)\n",
        "J_rev = jacrev(g)(u0)\n",
        "print('Jacobian (fwd):\\n', J_fwd)\n",
        "print('Jacobian (rev):\\n', J_rev)\n",
        "\n",
        "# custom_vjp for a stable log-sum-exp\n",
        "@jax.custom_vjp\n",
        "def stable_logsumexp(x):\n",
        "    m = jnp.max(x)\n",
        "    return m + jnp.log(jnp.sum(jnp.exp(x - m)))\n",
        "\n",
        "def slse_fwd(x):\n",
        "    y = stable_logsumexp(x)\n",
        "    return y, (x, y)\n",
        "\n",
        "def slse_bwd(res, g):\n",
        "    x, y = res\n",
        "    p = jnp.exp(x - y)  # softmax probabilities\n",
        "    return (g * p,)\n",
        "\n",
        "stable_logsumexp.defvjp(slse_fwd, slse_bwd)\n",
        "\n",
        "xx = jnp.array([1000.0, 999.0, 998.0])\n",
        "print('stable LSE:', stable_logsumexp(xx))\n",
        "print('grad check:', grad(lambda t: stable_logsumexp(t))(xx))\n"
      ],
      "metadata": {
        "id": "ZjmJrqwTSp03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Control flow (lax.cond, lax.scan, while_loop)"
      ],
      "metadata": {
        "id": "u0L094coSsQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cond\n",
        "def piecewise(x):\n",
        "    return lax.cond(x > 0, lambda t: t*t, lambda t: -t, x)\n",
        "\n",
        "print(piecewise(2.), piecewise(-3.))\n",
        "\n",
        "# scan: sum over a sequence (with state \"carry\")\n",
        "def scan_body(carry, x):\n",
        "    s = carry + x\n",
        "    return s, s  # (new_carry, stacked_output)\n",
        "\n",
        "xs = jnp.arange(5.0)\n",
        "carry0 = 0.0\n",
        "final_carry, s_hist = lax.scan(scan_body, carry0, xs)\n",
        "print('final sum:', final_carry)\n",
        "print('s_hist:', s_hist)\n",
        "\n",
        "# while_loop: simple countdown\n",
        "def cond_fun(state):\n",
        "    i, acc = state\n",
        "    return i > 0\n",
        "\n",
        "def body_fun(state):\n",
        "    i, acc = state\n",
        "    return (i-1, acc + i)\n",
        "\n",
        "init = (5, 0)\n",
        "i_end, acc = lax.while_loop(cond_fun, body_fun, init)\n",
        "print('while result:', i_end, acc)\n"
      ],
      "metadata": {
        "id": "R4m2TFaeStQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) LAX primitives, shape polymorphism, eval_shape"
      ],
      "metadata": {
        "id": "Dd-nAuhiSvHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_shape to probe shapes\n",
        "def my_net(params, x):\n",
        "    for W, b in params:\n",
        "        x = jnp.tanh(x @ W + b)\n",
        "    return x\n",
        "\n",
        "params = [(random.normal(random.PRNGKey(0), (32, 64)), jnp.zeros((64,))),\n",
        "          (random.normal(random.PRNGKey(1), (64, 10)), jnp.zeros((10,)))]\n",
        "sig = jax.eval_shape(my_net, params, jax.ShapeDtypeStruct((8, 32), jnp.float32))\n",
        "print(sig)\n",
        "\n",
        "# A tiny LAX convolution (N,H,W,C) * (KH,KW,C,OC)\n",
        "N,H,W,C = 1, 8, 8, 1\n",
        "x = random.normal(random.PRNGKey(0), (N,H,W,C))\n",
        "Wk = random.normal(random.PRNGKey(1), (3,3,C,4))\n",
        "y = lax.conv_general_dilated(x, Wk, window_strides=(1,1), padding='SAME')\n",
        "print('conv out shape:', y.shape)\n"
      ],
      "metadata": {
        "id": "i5_3qvaRSwDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGn55sKmSw-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Performance: tracing, static args, donation, timing, lower/HLO"
      ],
      "metadata": {
        "id": "RQPq25oWSx9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep shapes/dtypes steady to avoid retracing.\n",
        "\n",
        "Mark Python values as static when they change control-flow/structure.\n",
        "\n",
        "Use buffer donation to reduce copies.\n",
        "\n",
        "Always time with .block_until_ready().\n",
        "\n",
        "Peek at compiler IR via lower(...).as_text()."
      ],
      "metadata": {
        "id": "zkC4Fv0jS0XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Static args (e.g., a small Python \"mode\" flag)\n",
        "@jit\n",
        "def reg_loss(w, x, y, penalty: str = 'l2', lam: float = 1e-2):\n",
        "    logits = x @ w\n",
        "    nll = jnp.mean(jnp.logaddexp(0.0, -y * logits))\n",
        "    if penalty == 'l2':\n",
        "        reg = 0.5 * lam * jnp.sum(w * w)\n",
        "    elif penalty == 'l1':\n",
        "        reg = lam * jnp.sum(jnp.abs(w))\n",
        "    else:\n",
        "        raise ValueError('unknown penalty')\n",
        "    return nll + reg\n",
        "\n",
        "w = jnp.zeros((10,))\n",
        "x = random.normal(random.PRNGKey(0), (128, 10))\n",
        "y = jnp.sign(random.normal(random.PRNGKey(1), (128,)))\n",
        "print(reg_loss(w, x, y, 'l2', 1e-2))\n",
        "\n",
        "# Donation example (donate_argnums=0 donates first arg \"arr\")\n",
        "@jit(donate_argnums=(0,))\n",
        "def in_placey_add(arr, val):\n",
        "    return arr + val\n",
        "\n",
        "arr = jnp.ones((1_000_000,))\n",
        "t0 = time.time(); arr = in_placey_add(arr, 1).block_until_ready(); t1 = time.time()\n",
        "print('donated add ms:', (t1-t0)*1e3)\n",
        "\n",
        "# Inspect lowered IR/HLO (API may vary slightly across versions)\n",
        "def foo_to_lower(a, b):\n",
        "    return a @ b\n",
        "\n",
        "print(\n",
        "    jit(foo_to_lower).lower(jnp.ones((64,64), jnp.float32), jnp.ones((64,64), jnp.float32)).as_text()[:500]\n",
        ")\n"
      ],
      "metadata": {
        "id": "_5gtICIMSy1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4VhXMPGS1gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorization & parallelism (vmap, pmap (legacy), intro to pjit & sharding)"
      ],
      "metadata": {
        "id": "oLwKHnshS2q_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_X8PZwOS3r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vmap: auto-batch on one device.\n",
        "\n",
        "pmap: SPMD across devices (still useful, esp. TPUs).\n",
        "\n",
        "pjit + explicit sharding with Mesh/PartitionSpec (recommended for new multi-device work)."
      ],
      "metadata": {
        "id": "J1sDD032S4WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vmap: pairwise dot without Python loops\n",
        "def dot(a, b):  # [..., d] x [..., d] -> [...]\n",
        "    return jnp.sum(a * b, axis=-1)\n",
        "\n",
        "A = random.normal(random.PRNGKey(0), (64, 128))\n",
        "B = random.normal(random.PRNGKey(1), (64, 128))\n",
        "batched_dot = vmap(dot)\n",
        "print('batched dot shape:', batched_dot(A, B).shape)\n",
        "\n",
        "# pmap demo (will degenerate to single-device behavior if you only have 1 device)\n",
        "@jax.pmap\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "xs = jnp.arange(jax.device_count())\n",
        "print('device_count =', jax.device_count())\n",
        "print('pmap result:', add_one(xs))\n",
        "\n",
        "# pjit-style sharding (pattern only; single-device becomes no-op)\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "\n",
        "devices = np.array(jax.devices())\n",
        "mesh = Mesh(devices[:max(1, min(2, devices.size))], ('data',))\n",
        "print('Mesh axes:', mesh.axis_names)\n"
      ],
      "metadata": {
        "id": "4e1-HmD9S40O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Numerics & mixed precision (stable softmax, gradient clipping)\n",
        "\n",
        "Use numerically stable forms; consider mixed precision (fp16/bfloat16 for matmuls/conv, fp32 for accumulators)."
      ],
      "metadata": {
        "id": "CddEDnJOS7nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=-1):\n",
        "    z = x - jnp.max(x, axis=axis, keepdims=True)\n",
        "    num = jnp.exp(z)\n",
        "    return num / jnp.sum(num, axis=axis, keepdims=True)\n",
        "\n",
        "x = jnp.array([[1000., 0., -1000.]])\n",
        "print('stable softmax:', softmax(x))\n",
        "\n",
        "# Simple global-norm clip util\n",
        "def clip_by_global_norm(tree, max_norm=1.0):\n",
        "    gsq = sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(tree)])\n",
        "    scale = jnp.minimum(1.0, max_norm / (jnp.sqrt(gsq) + 1e-8))\n",
        "    return jax.tree_map(lambda g: g * scale, tree)\n"
      ],
      "metadata": {
        "id": "5gMdU0RXS89u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) End-to-end MLP (from scratch)\n",
        "\n",
        "Synthetic 2-class data, MLP, training step with jit."
      ],
      "metadata": {
        "id": "ipUWrfXPS9P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_toy_data(k, n_per_class=512, spread=0.7, seed=0):\n",
        "    key = random.PRNGKey(seed)\n",
        "    key1, key2 = random.split(key)\n",
        "    c0 = random.normal(key1, (n_per_class, 2)) * spread + jnp.array([0., -k])\n",
        "    c1 = random.normal(key2, (n_per_class, 2)) * spread + jnp.array([0., k])\n",
        "    X = jnp.concatenate([c0, c1], axis=0)\n",
        "    y = jnp.concatenate([-jnp.ones((n_per_class,)), jnp.ones((n_per_class,))], axis=0)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_toy_data(2.5)\n",
        "\n",
        "def init_mlp(key, sizes):\n",
        "    keys = random.split(key, len(sizes)-1)\n",
        "    params = []\n",
        "    for k, (m, n) in zip(keys, zip(sizes[:-1], sizes[1:])):\n",
        "        W = random.normal(k, (m, n)) / jnp.sqrt(m)\n",
        "        b = jnp.zeros((n,))\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def mlp_apply(params, x):\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.tanh(x @ W + b)\n",
        "    W, b = params[-1]\n",
        "    return x @ W + b  # logits\n",
        "\n",
        "def binary_loss(params, x, y):\n",
        "    logits = mlp_apply(params, x).squeeze(-1)\n",
        "    return jnp.mean(jnp.logaddexp(0.0, -y * logits))  # logistic loss\n",
        "\n",
        "@jit\n",
        "def train_step(params, x, y, lr=1e-2):\n",
        "    loss, grads = value_and_grad(binary_loss)(params, x, y)\n",
        "    # grads = clip_by_global_norm(grads, 1.0)  # optional\n",
        "    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "    return params, loss\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "params = init_mlp(key, [2, 64, 64, 1])\n",
        "\n",
        "losses = []\n",
        "for step in range(400):\n",
        "    params, l = train_step(params, X, y)\n",
        "    losses.append(float(l))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.title('MLP Training Loss')\n",
        "plt.xlabel('step'); plt.ylabel('loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4gQ6GDxmS-8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11) CNN (from scratch, synthetic images)"
      ],
      "metadata": {
        "id": "v0XDLpRITA7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake images (N, H, W, C)\n",
        "N, H, W, C = 64, 28, 28, 1\n",
        "key = random.PRNGKey(1)\n",
        "imgs = random.normal(key, (N, H, W, C))\n",
        "labels = jnp.where(jnp.mean(imgs, axis=(1,2,3)) > 0, 1., -1.)  # separable synthetic target\n",
        "\n",
        "def init_cnn(key):\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "    W1 = random.normal(k1, (3, 3, C, 8)) / jnp.sqrt(3*3*C)\n",
        "    b1 = jnp.zeros((8,))\n",
        "    W2 = random.normal(k2, (3, 3, 8, 16)) / jnp.sqrt(3*3*8)\n",
        "    b2 = jnp.zeros((16,))\n",
        "    W3 = random.normal(k3, (16*7*7, 1)) / jnp.sqrt(16*7*7)\n",
        "    b3 = jnp.zeros((1,))\n",
        "    return (W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "def cnn_apply(params, x):\n",
        "    W1,b1,W2,b2,W3,b3 = params\n",
        "    y = lax.conv_general_dilated(x, W1, (1,1), 'SAME')\n",
        "    y = jnp.tanh(y + b1)\n",
        "    y = lax.conv_general_dilated(y, W2, (2,2), 'SAME')  # stride 2\n",
        "    y = jnp.tanh(y + b2)\n",
        "    y = y.reshape((y.shape[0], -1))\n",
        "    y = y @ W3 + b3\n",
        "    return y.squeeze(-1)\n",
        "\n",
        "@jit\n",
        "def cnn_loss(params, x, y):\n",
        "    logits = cnn_apply(params, x)\n",
        "    return jnp.mean(jnp.logaddexp(0.0, -y * logits))\n",
        "\n",
        "@jit\n",
        "def cnn_step(params, x, y, lr=1e-2):\n",
        "    l, g = value_and_grad(cnn_loss)(params, x, y)\n",
        "    params = jax.tree_map(lambda p, gg: p - lr * gg, params, g)\n",
        "    return params, l\n",
        "\n",
        "params_cnn = init_cnn(random.PRNGKey(2))\n",
        "for step in range(200):\n",
        "    params_cnn, l = cnn_step(params_cnn, imgs, labels)\n",
        "print('Final CNN loss:', float(l))\n"
      ],
      "metadata": {
        "id": "Kpz_xCwNTCsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12) RNN via lax.scan (sequence modeling)"
      ],
      "metadata": {
        "id": "O2xCvL98TDGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T, B, D_in, D_hid = 50, 32, 16, 32\n",
        "key = random.PRNGKey(3)\n",
        "xs = random.normal(key, (T, B, D_in))\n",
        "ys = random.normal(key, (T, B, D_hid))\n",
        "\n",
        "def init_rnn(key):\n",
        "    k1,k2,k3 = random.split(key, 3)\n",
        "    params = dict(\n",
        "        Wx = random.normal(k1, (D_in, D_hid)) / jnp.sqrt(D_in),\n",
        "        Wh = random.normal(k2, (D_hid, D_hid)) / jnp.sqrt(D_hid),\n",
        "        b  = jnp.zeros((D_hid,))\n",
        "    )\n",
        "    return params\n",
        "\n",
        "def rnn_step(params, h, x):\n",
        "    h_new = jnp.tanh(x @ params['Wx'] + h @ params['Wh'] + params['b'])\n",
        "    return h_new, h_new\n",
        "\n",
        "def rnn_apply(params, x_seq, h0):\n",
        "    hT, hs = lax.scan(lambda h, x: rnn_step(params, h, x), h0, x_seq)\n",
        "    return hT, hs\n",
        "\n",
        "params_rnn = init_rnn(key)\n",
        "h0 = jnp.zeros((B, D_hid))\n",
        "_, hs = rnn_apply(params_rnn, xs, h0)\n",
        "print('RNN hidden seq shape:', hs.shape)  # (T, B, D_hid)\n"
      ],
      "metadata": {
        "id": "3wRz6QF5TE73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13) Compact Transformer encoder block (no Flax)"
      ],
      "metadata": {
        "id": "MFJnvbR5TGLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(x, eps=1e-5):\n",
        "    m = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    v = jnp.mean((x - m)**2, axis=-1, keepdims=True)\n",
        "    return (x - m) / jnp.sqrt(v + eps)\n",
        "\n",
        "def init_transformer_block(key, d_model=128, n_heads=4, d_ff=256):\n",
        "    kq, kk, kv, ko, k1, k2 = random.split(key, 6)\n",
        "    params = dict(\n",
        "        Wq = random.normal(kq, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wk = random.normal(kk, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wv = random.normal(kv, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wo = random.normal(ko, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        W1 = random.normal(k1, (d_model, d_ff)) / jnp.sqrt(d_model),\n",
        "        W2 = random.normal(k2, (d_ff, d_model)) / jnp.sqrt(d_ff),\n",
        "        b1 = jnp.zeros((d_ff,)),\n",
        "        b2 = jnp.zeros((d_model,)),\n",
        "    )\n",
        "    head_dim = d_model // n_heads\n",
        "    return params, head_dim, n_heads\n",
        "\n",
        "def split_heads(x, n_heads):\n",
        "    B,T,D = x.shape\n",
        "    return x.reshape(B, T, n_heads, D//n_heads).transpose(0,2,1,3)  # (B,H,T,dh)\n",
        "\n",
        "def combine_heads(x):\n",
        "    B,H,T,dh = x.shape\n",
        "    return x.transpose(0,2,1,3).reshape(B, T, H*dh)\n",
        "\n",
        "def mhsa(params, x, head_dim, n_heads, mask=None):\n",
        "    Q = x @ params['Wq']; K = x @ params['Wk']; V = x @ params['Wv']\n",
        "    Qh, Kh, Vh = split_heads(Q, n_heads), split_heads(K, n_heads), split_heads(V, n_heads)\n",
        "    scale = 1.0 / jnp.sqrt(head_dim)\n",
        "    attn = jnp.einsum('bhtd,bhTd->bhtT', Qh, Kh) * scale  # (B,H,T,T)\n",
        "    if mask is not None:\n",
        "        attn = jnp.where(mask, attn, -1e9)\n",
        "    probs = softmax(attn, axis=-1)\n",
        "    out = jnp.einsum('bhtT,bhTd->bhtd', probs, Vh)\n",
        "    out = combine_heads(out) @ params['Wo']\n",
        "    return out\n",
        "\n",
        "def transformer_block(params, x, head_dim, n_heads, mask=None):\n",
        "    x = layer_norm(x + mhsa(params, x, head_dim, n_heads, mask))\n",
        "    y = jnp.tanh(x @ params['W1'] + params['b1'])\n",
        "    y = y @ params['W2'] + params['b2']\n",
        "    x = layer_norm(x + y)\n",
        "    return x\n",
        "\n",
        "# Demo\n",
        "B, T, D = 8, 32, 128\n",
        "key = random.PRNGKey(0)\n",
        "x = random.normal(key, (B, T, D))\n",
        "params_tx, head_dim, n_heads = init_transformer_block(key, d_model=D, n_heads=4, d_ff=256)\n",
        "y = transformer_block(params_tx, x, head_dim, n_heads)\n",
        "print('Transformer out:', y.shape)\n"
      ],
      "metadata": {
        "id": "JLk8VV37TG5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14) Optimization with Optax (adamw, schedules, clipping)"
      ],
      "metadata": {
        "id": "Zsfyi3A2TI1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import optax\n",
        "\n",
        "    def make_optimizer(lr=1e-3, wd=1e-4):\n",
        "        return optax.chain(\n",
        "            optax.clip_by_global_norm(1.0),\n",
        "            optax.adamw(learning_rate=lr, weight_decay=wd)\n",
        "        )\n",
        "\n",
        "    @dataclass\n",
        "    class TrainState:\n",
        "        params: Any\n",
        "        opt_state: Any\n",
        "\n",
        "    def init_train_state(params, opt):\n",
        "        return TrainState(params=params, opt_state=opt.init(params))\n",
        "\n",
        "    def apply_updates(params, updates):\n",
        "        return optax.apply_updates(params, updates)\n",
        "\n",
        "    @jit\n",
        "    def train_step_optax(state, x, y, opt):\n",
        "        loss, grads = value_and_grad(binary_loss)(state.params, x, y)\n",
        "        updates, new_opt_state = opt.update(grads, state.opt_state, state.params)\n",
        "        new_params = apply_updates(state.params, updates)\n",
        "        return TrainState(new_params, new_opt_state), loss\n",
        "\n",
        "    # Demo on MLP data\n",
        "    key = random.PRNGKey(0)\n",
        "    params0 = init_mlp(key, [2, 64, 64, 1])\n",
        "    opt = make_optimizer(1e-2, 1e-4)\n",
        "    state = init_train_state(params0, opt)\n",
        "    for step in range(200):\n",
        "        state, l = train_step_optax(state, X, y, opt)\n",
        "    print('Optax demo loss:', float(l))\n",
        "\n",
        "except Exception as e:\n",
        "    print('Optax not available:', e)\n"
      ],
      "metadata": {
        "id": "UhA6H-hoTKXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory: rematerialization (remat/checkpoint), scans vs vmaps, donation"
      ],
      "metadata": {
        "id": "gF5S67ObTMMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rematerialization: recompute activations during backward to save memory\n",
        "from jax import checkpoint as remat\n",
        "\n",
        "def big_chain(x, depth=10):\n",
        "    for _ in range(depth):\n",
        "        x = jnp.tanh(x @ x.T)\n",
        "    return x\n",
        "\n",
        "@jit\n",
        "def f_no_remat(x): return big_chain(x)\n",
        "\n",
        "@jit\n",
        "def f_with_remat(x): return remat(big_chain)(x)\n",
        "\n",
        "Xbig = random.normal(random.PRNGKey(0), (128, 128))\n",
        "_ = f_no_remat(Xbig).block_until_ready()\n",
        "_ = f_with_remat(Xbig).block_until_ready()\n",
        "print('Remat demo complete (profile on your hardware for real gains).')\n"
      ],
      "metadata": {
        "id": "UsRQ2um2TN8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOypR5LHTPGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving / loading params (simple & robust)"
      ],
      "metadata": {
        "id": "_JS2yU4ZTQjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Pickle (simple)\n",
        "def tree_save_pickle(path, pytree):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(pytree, f)\n",
        "\n",
        "def tree_load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "tree_save_pickle('params_demo.pkl', params)\n",
        "params_loaded = tree_load_pickle('params_demo.pkl')\n",
        "print('pickle load ok:', jax.tree_util.tree_structure(params) == jax.tree_util.tree_structure(params_loaded))\n",
        "\n",
        "# 2) NPZ with treedef (object array)\n",
        "from jax import tree_util\n",
        "\n",
        "def tree_save_npz(path, pytree):\n",
        "    leaves, treedef = tree_util.tree_flatten(pytree)\n",
        "    save_dict = {f'leaf_{i}': np.asarray(leaf) for i, leaf in enumerate(leaves)}\n",
        "    save_dict['treedef'] = np.array([treedef], dtype=object)\n",
        "    np.savez(path, **save_dict)\n",
        "\n",
        "def tree_load_npz(path):\n",
        "    with np.load(path, allow_pickle=True) as data:\n",
        "        treedef = data['treedef'][0]\n",
        "        leaf_keys = sorted([k for k in data.files if k.startswith('leaf_')],\n",
        "                           key=lambda s: int(s.split('_')[1]))\n",
        "        leaves = [data[k] for k in leaf_keys]\n",
        "    return tree_util.tree_unflatten(treedef, leaves)\n",
        "\n",
        "tree_save_npz('params_demo.npz', params)\n",
        "params2 = tree_load_npz('params_demo.npz')\n",
        "print('npz load ok:', jax.tree_util.tree_structure(params) == jax.tree_util.tree_structure(params2))\n"
      ],
      "metadata": {
        "id": "082g7kZGTRZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "got it — here’s a self-contained, section-by-section JAX “from zero to expert” you can paste straight into a Jupyter notebook. each section has short notes + runnable code. if anything’s unclear, ping me and we’ll go deeper.\n",
        "\n",
        "---\n",
        "\n",
        "# JAX “From Zero to Expert”\n",
        "\n",
        "> hands-on coverage of: JAX basics (jit/grad/vmap), pytrees, PRNG best practices; autodiff theory & APIs (jvp/vjp, jacfwd/jacrev, custom rules); control flow; LAX primitives; performance (tracing, static args, donation, remat, timing); vectorization & parallelism (vmap, pmap, intro to pjit & sharding); numerics & mixed precision; end-to-end models (MLP, CNN, RNN with scan, compact Transformer block); optimization with Optax; saving/loading; debugging/profiling; interop — plus a few advanced extras.\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Setup (run first)\n",
        "\n",
        "**Tip:** pick the correct JAX wheel for your hardware. CPU is simplest; CUDA wheels depend on your CUDA version.\n",
        "\n",
        "```python\n",
        "# If you're on CPU-only:\n",
        "# %pip install -U \"jax[cpu]\"\n",
        "\n",
        "# If you have CUDA (example for CUDA 12):\n",
        "# %pip install -U \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# Optional but recommended for sections below:\n",
        "# %pip install -U optax flax orbax-checkpoint matplotlib\n",
        "\n",
        "import os, sys, math, time, functools, itertools, pickle\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Tuple, Dict\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, value_and_grad, random, lax, vmap, jacrev, jacfwd\n",
        "import numpy as np\n",
        "\n",
        "print('JAX version:', jax.__version__)\n",
        "print('Backend:', jax.default_backend())\n",
        "print('Devices:', jax.devices())\n",
        "\n",
        "# Optional helpful toggles:\n",
        "from jax import config as jax_config\n",
        "# jax_config.update('jax_enable_x64', True)    # Enable 64-bit if needed (slower on GPU)\n",
        "# os.environ['JAX_DEBUG_NANS'] = 'true'        # Extra NaN checks inside compiled code\n",
        "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'  # Avoid preallocating full GPU memory\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 1) JAX basics (arrays, no in-place updates, jit, grad, vmap)\n",
        "\n",
        "Key ideas: functional transforms (`jit`, `grad`, `vmap`) act on **functions**; no in-place mutation (use `.at[...]`); first `jit` call compiles; timings should block with `.block_until_ready()`.\n",
        "\n",
        "```python\n",
        "# Arrays & broadcasting\n",
        "x = jnp.arange(6).reshape(2, 3)\n",
        "y = jnp.ones((3,))\n",
        "print('x:\\n', x)\n",
        "print('x + y:\\n', x + y)  # broadcasting\n",
        "\n",
        "# No in-place mutation -> use .at\n",
        "z = x.at[:, 1].set(42)\n",
        "print('z (col 1 set to 42):\\n', z)\n",
        "\n",
        "# JIT compilation: first call compiles, subsequent calls are fast\n",
        "@jit\n",
        "def poly(a, b, x):\n",
        "    return a * x**2 + b * x + 1.0\n",
        "\n",
        "xv = jnp.linspace(-3, 3, 100_000)\n",
        "a, b = 2.0, -1.0\n",
        "\n",
        "t0 = time.time()\n",
        "out = poly(a, b, xv).block_until_ready()  # compile + run\n",
        "t1 = time.time()\n",
        "out2 = poly(a, b, xv).block_until_ready() # run-only\n",
        "t2 = time.time()\n",
        "print(f'First call (compile+run): {(t1 - t0)*1e3:.2f} ms')\n",
        "print(f'Second call (run only):   {(t2 - t1)*1e3:.2f} ms')\n",
        "\n",
        "# Autodiff with grad\n",
        "def loss_fn(w, x, y):\n",
        "    pred = w[0] * x + w[1]\n",
        "    return jnp.mean((pred - y)**2)\n",
        "\n",
        "w = jnp.array([2.0, -1.0])\n",
        "x = jnp.linspace(-1, 1, 1000)\n",
        "y = 3.0 * x + 0.5  # true slope=3, intercept=0.5\n",
        "\n",
        "dloss_dw = grad(loss_fn)(w, x, y)\n",
        "print('Gradients:', dloss_dw)\n",
        "\n",
        "# vmap to batch without Python loops\n",
        "def square_plus_one(t):\n",
        "    return t*t + 1\n",
        "\n",
        "batched_square = vmap(square_plus_one)  # vectorize over leading axis\n",
        "print('vmap:', batched_square(jnp.arange(5.)))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Pytrees (structuring parameters)\n",
        "\n",
        "Pytree = nested structure of leaves (arrays/scalars). Many JAX APIs accept pytrees directly.\n",
        "\n",
        "```python\n",
        "from jax import tree_util\n",
        "\n",
        "params = {'W': jnp.ones((3, 3)), 'b': jnp.zeros((3,))}\n",
        "print('Leaves:', tree_util.tree_leaves(params))\n",
        "\n",
        "# Map over leaves\n",
        "scaled = jax.tree_map(lambda x: 0.5 * x, params)\n",
        "print('Scaled b:', scaled['b'])\n",
        "\n",
        "# dataclass as pytree\n",
        "@dataclass\n",
        "class MLPParams:\n",
        "    W1: jnp.ndarray\n",
        "    b1: jnp.ndarray\n",
        "    W2: jnp.ndarray\n",
        "    b2: jnp.ndarray\n",
        "\n",
        "p = MLPParams(jnp.ones((4, 8)), jnp.zeros((8,)), jnp.ones((8, 2)), jnp.zeros((2,)))\n",
        "print('Num leaves:', len(tree_util.tree_leaves(p)))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3) PRNG best practices (functional randomness)\n",
        "\n",
        "Split keys; treat keys as inputs/outputs; use `fold_in(step)` for reproducible per-step seeding.\n",
        "\n",
        "```python\n",
        "key = random.PRNGKey(42)\n",
        "key, k1, k2 = random.split(key, 3)\n",
        "print('Normal:', random.normal(k1, (3,)))\n",
        "print('Uniform:', random.uniform(k2, (3,)))\n",
        "\n",
        "# fold_in for deterministic per-step randomness\n",
        "base = random.PRNGKey(123)\n",
        "for step in range(3):\n",
        "    k = random.fold_in(base, step)\n",
        "    print('step', step, '->', random.normal(k, ()).item())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Autodiff theory + APIs (jvp/vjp, jacfwd/jacrev, custom\\_vjp/jvp)\n",
        "\n",
        "* Forward-mode AD (`jvp`, `jacfwd`) is efficient when input dim ≤ output dim.\n",
        "* Reverse-mode AD (`grad`, `vjp`, `jacrev`) is efficient when output dim ≤ input dim.\n",
        "* You can nest them for higher-order derivatives.\n",
        "* Custom rules (`custom_vjp` / `custom_jvp`) = numerical stability or non-standard ops.\n",
        "\n",
        "```python\n",
        "# jvp/vjp example\n",
        "def f(u):\n",
        "    return jnp.array([u[0]*jnp.sin(u[1]), u[0]*jnp.cos(u[1])])  # R^2 -> R^2\n",
        "\n",
        "u0 = jnp.array([2.0, 0.3])\n",
        "tangent = jnp.array([1.0, -0.2])\n",
        "\n",
        "y, jvp_val = jax.jvp(f, (u0,), (tangent,))\n",
        "print('f(u0)=', y)\n",
        "print('JVP(f; u0, tangent)=', jvp_val)\n",
        "\n",
        "y, pullback = jax.vjp(f, u0)\n",
        "bar = jnp.array([1.0, 0.0])  # seed cotangent\n",
        "vjp_val = pullback(bar)[0]\n",
        "print('VJP(f; u0, bar)=', vjp_val)\n",
        "\n",
        "# Jacobians\n",
        "def g(u):\n",
        "    return jnp.array([u[0]**2 + u[1], jnp.sin(u[0]*u[1])])\n",
        "\n",
        "J_fwd = jacfwd(g)(u0)\n",
        "J_rev = jacrev(g)(u0)\n",
        "print('Jacobian (fwd):\\n', J_fwd)\n",
        "print('Jacobian (rev):\\n', J_rev)\n",
        "\n",
        "# custom_vjp for a stable log-sum-exp\n",
        "@jax.custom_vjp\n",
        "def stable_logsumexp(x):\n",
        "    m = jnp.max(x)\n",
        "    return m + jnp.log(jnp.sum(jnp.exp(x - m)))\n",
        "\n",
        "def slse_fwd(x):\n",
        "    y = stable_logsumexp(x)\n",
        "    return y, (x, y)\n",
        "\n",
        "def slse_bwd(res, g):\n",
        "    x, y = res\n",
        "    p = jnp.exp(x - y)  # softmax probabilities\n",
        "    return (g * p,)\n",
        "\n",
        "stable_logsumexp.defvjp(slse_fwd, slse_bwd)\n",
        "\n",
        "xx = jnp.array([1000.0, 999.0, 998.0])\n",
        "print('stable LSE:', stable_logsumexp(xx))\n",
        "print('grad check:', grad(lambda t: stable_logsumexp(t))(xx))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Control flow (lax.cond, lax.scan, while\\_loop)\n",
        "\n",
        "Prefer JAX control-flow primitives **inside jitted code**.\n",
        "\n",
        "```python\n",
        "# cond\n",
        "def piecewise(x):\n",
        "    return lax.cond(x > 0, lambda t: t*t, lambda t: -t, x)\n",
        "\n",
        "print(piecewise(2.), piecewise(-3.))\n",
        "\n",
        "# scan: sum over a sequence (with state \"carry\")\n",
        "def scan_body(carry, x):\n",
        "    s = carry + x\n",
        "    return s, s  # (new_carry, stacked_output)\n",
        "\n",
        "xs = jnp.arange(5.0)\n",
        "carry0 = 0.0\n",
        "final_carry, s_hist = lax.scan(scan_body, carry0, xs)\n",
        "print('final sum:', final_carry)\n",
        "print('s_hist:', s_hist)\n",
        "\n",
        "# while_loop: simple countdown\n",
        "def cond_fun(state):\n",
        "    i, acc = state\n",
        "    return i > 0\n",
        "\n",
        "def body_fun(state):\n",
        "    i, acc = state\n",
        "    return (i-1, acc + i)\n",
        "\n",
        "init = (5, 0)\n",
        "i_end, acc = lax.while_loop(cond_fun, body_fun, init)\n",
        "print('while result:', i_end, acc)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6) LAX primitives, shape polymorphism, eval\\_shape\n",
        "\n",
        "LAX exposes low-level ops (`lax.dot_general`, `lax.conv_general_dilated`). Use `jax.eval_shape` to inspect shapes/dtypes **without allocating**.\n",
        "\n",
        "```python\n",
        "# eval_shape to probe shapes\n",
        "def my_net(params, x):\n",
        "    for W, b in params:\n",
        "        x = jnp.tanh(x @ W + b)\n",
        "    return x\n",
        "\n",
        "params = [(random.normal(random.PRNGKey(0), (32, 64)), jnp.zeros((64,))),\n",
        "          (random.normal(random.PRNGKey(1), (64, 10)), jnp.zeros((10,)))]\n",
        "sig = jax.eval_shape(my_net, params, jax.ShapeDtypeStruct((8, 32), jnp.float32))\n",
        "print(sig)\n",
        "\n",
        "# A tiny LAX convolution (N,H,W,C) * (KH,KW,C,OC)\n",
        "N,H,W,C = 1, 8, 8, 1\n",
        "x = random.normal(random.PRNGKey(0), (N,H,W,C))\n",
        "Wk = random.normal(random.PRNGKey(1), (3,3,C,4))\n",
        "y = lax.conv_general_dilated(x, Wk, window_strides=(1,1), padding='SAME')\n",
        "print('conv out shape:', y.shape)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Performance: tracing, static args, donation, timing, lower/HLO\n",
        "\n",
        "* Keep shapes/dtypes steady to avoid retracing.\n",
        "* Mark Python values as **static** when they change control-flow/structure.\n",
        "* Use **buffer donation** to reduce copies.\n",
        "* Always time with `.block_until_ready()`.\n",
        "* Peek at compiler IR via `lower(...).as_text()`.\n",
        "\n",
        "```python\n",
        "# Static args (e.g., a small Python \"mode\" flag)\n",
        "@jit\n",
        "def reg_loss(w, x, y, penalty: str = 'l2', lam: float = 1e-2):\n",
        "    logits = x @ w\n",
        "    nll = jnp.mean(jnp.logaddexp(0.0, -y * logits))\n",
        "    if penalty == 'l2':\n",
        "        reg = 0.5 * lam * jnp.sum(w * w)\n",
        "    elif penalty == 'l1':\n",
        "        reg = lam * jnp.sum(jnp.abs(w))\n",
        "    else:\n",
        "        raise ValueError('unknown penalty')\n",
        "    return nll + reg\n",
        "\n",
        "w = jnp.zeros((10,))\n",
        "x = random.normal(random.PRNGKey(0), (128, 10))\n",
        "y = jnp.sign(random.normal(random.PRNGKey(1), (128,)))\n",
        "print(reg_loss(w, x, y, 'l2', 1e-2))\n",
        "\n",
        "# Donation example (donate_argnums=0 donates first arg \"arr\")\n",
        "@jit(donate_argnums=(0,))\n",
        "def in_placey_add(arr, val):\n",
        "    return arr + val\n",
        "\n",
        "arr = jnp.ones((1_000_000,))\n",
        "t0 = time.time(); arr = in_placey_add(arr, 1).block_until_ready(); t1 = time.time()\n",
        "print('donated add ms:', (t1-t0)*1e3)\n",
        "\n",
        "# Inspect lowered IR/HLO (API may vary slightly across versions)\n",
        "def foo_to_lower(a, b):\n",
        "    return a @ b\n",
        "\n",
        "print(\n",
        "    jit(foo_to_lower).lower(jnp.ones((64,64), jnp.float32), jnp.ones((64,64), jnp.float32)).as_text()[:500]\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Vectorization & parallelism (vmap, pmap (legacy), intro to pjit & sharding)\n",
        "\n",
        "* `vmap`: auto-batch on one device.\n",
        "* `pmap`: SPMD across devices (still useful, esp. TPUs).\n",
        "* `pjit` + **explicit sharding** with Mesh/PartitionSpec (recommended for new multi-device work).\n",
        "\n",
        "```python\n",
        "# vmap: pairwise dot without Python loops\n",
        "def dot(a, b):  # [..., d] x [..., d] -> [...]\n",
        "    return jnp.sum(a * b, axis=-1)\n",
        "\n",
        "A = random.normal(random.PRNGKey(0), (64, 128))\n",
        "B = random.normal(random.PRNGKey(1), (64, 128))\n",
        "batched_dot = vmap(dot)\n",
        "print('batched dot shape:', batched_dot(A, B).shape)\n",
        "\n",
        "# pmap demo (will degenerate to single-device behavior if you only have 1 device)\n",
        "@jax.pmap\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "xs = jnp.arange(jax.device_count())\n",
        "print('device_count =', jax.device_count())\n",
        "print('pmap result:', add_one(xs))\n",
        "\n",
        "# pjit-style sharding (pattern only; single-device becomes no-op)\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "\n",
        "devices = np.array(jax.devices())\n",
        "mesh = Mesh(devices[:max(1, min(2, devices.size))], ('data',))\n",
        "print('Mesh axes:', mesh.axis_names)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Numerics & mixed precision (stable softmax, gradient clipping)\n",
        "\n",
        "Use numerically stable forms; consider mixed precision (fp16/bfloat16 for matmuls/conv, fp32 for accumulators).\n",
        "\n",
        "```python\n",
        "def softmax(x, axis=-1):\n",
        "    z = x - jnp.max(x, axis=axis, keepdims=True)\n",
        "    num = jnp.exp(z)\n",
        "    return num / jnp.sum(num, axis=axis, keepdims=True)\n",
        "\n",
        "x = jnp.array([[1000., 0., -1000.]])\n",
        "print('stable softmax:', softmax(x))\n",
        "\n",
        "# Simple global-norm clip util\n",
        "def clip_by_global_norm(tree, max_norm=1.0):\n",
        "    gsq = sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(tree)])\n",
        "    scale = jnp.minimum(1.0, max_norm / (jnp.sqrt(gsq) + 1e-8))\n",
        "    return jax.tree_map(lambda g: g * scale, tree)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 10) End-to-end MLP (from scratch)\n",
        "\n",
        "Synthetic 2-class data, MLP, training step with `jit`.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_toy_data(k, n_per_class=512, spread=0.7, seed=0):\n",
        "    key = random.PRNGKey(seed)\n",
        "    key1, key2 = random.split(key)\n",
        "    c0 = random.normal(key1, (n_per_class, 2)) * spread + jnp.array([0., -k])\n",
        "    c1 = random.normal(key2, (n_per_class, 2)) * spread + jnp.array([0., k])\n",
        "    X = jnp.concatenate([c0, c1], axis=0)\n",
        "    y = jnp.concatenate([-jnp.ones((n_per_class,)), jnp.ones((n_per_class,))], axis=0)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_toy_data(2.5)\n",
        "\n",
        "def init_mlp(key, sizes):\n",
        "    keys = random.split(key, len(sizes)-1)\n",
        "    params = []\n",
        "    for k, (m, n) in zip(keys, zip(sizes[:-1], sizes[1:])):\n",
        "        W = random.normal(k, (m, n)) / jnp.sqrt(m)\n",
        "        b = jnp.zeros((n,))\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def mlp_apply(params, x):\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.tanh(x @ W + b)\n",
        "    W, b = params[-1]\n",
        "    return x @ W + b  # logits\n",
        "\n",
        "def binary_loss(params, x, y):\n",
        "    logits = mlp_apply(params, x).squeeze(-1)\n",
        "    return jnp.mean(jnp.logaddexp(0.0, -y * logits))  # logistic loss\n",
        "\n",
        "@jit\n",
        "def train_step(params, x, y, lr=1e-2):\n",
        "    loss, grads = value_and_grad(binary_loss)(params, x, y)\n",
        "    # grads = clip_by_global_norm(grads, 1.0)  # optional\n",
        "    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "    return params, loss\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "params = init_mlp(key, [2, 64, 64, 1])\n",
        "\n",
        "losses = []\n",
        "for step in range(400):\n",
        "    params, l = train_step(params, X, y)\n",
        "    losses.append(float(l))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.title('MLP Training Loss')\n",
        "plt.xlabel('step'); plt.ylabel('loss')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 11) CNN (from scratch, synthetic images)\n",
        "\n",
        "```python\n",
        "# Fake images (N, H, W, C)\n",
        "N, H, W, C = 64, 28, 28, 1\n",
        "key = random.PRNGKey(1)\n",
        "imgs = random.normal(key, (N, H, W, C))\n",
        "labels = jnp.where(jnp.mean(imgs, axis=(1,2,3)) > 0, 1., -1.)  # separable synthetic target\n",
        "\n",
        "def init_cnn(key):\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "    W1 = random.normal(k1, (3, 3, C, 8)) / jnp.sqrt(3*3*C)\n",
        "    b1 = jnp.zeros((8,))\n",
        "    W2 = random.normal(k2, (3, 3, 8, 16)) / jnp.sqrt(3*3*8)\n",
        "    b2 = jnp.zeros((16,))\n",
        "    W3 = random.normal(k3, (16*7*7, 1)) / jnp.sqrt(16*7*7)\n",
        "    b3 = jnp.zeros((1,))\n",
        "    return (W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "def cnn_apply(params, x):\n",
        "    W1,b1,W2,b2,W3,b3 = params\n",
        "    y = lax.conv_general_dilated(x, W1, (1,1), 'SAME')\n",
        "    y = jnp.tanh(y + b1)\n",
        "    y = lax.conv_general_dilated(y, W2, (2,2), 'SAME')  # stride 2\n",
        "    y = jnp.tanh(y + b2)\n",
        "    y = y.reshape((y.shape[0], -1))\n",
        "    y = y @ W3 + b3\n",
        "    return y.squeeze(-1)\n",
        "\n",
        "@jit\n",
        "def cnn_loss(params, x, y):\n",
        "    logits = cnn_apply(params, x)\n",
        "    return jnp.mean(jnp.logaddexp(0.0, -y * logits))\n",
        "\n",
        "@jit\n",
        "def cnn_step(params, x, y, lr=1e-2):\n",
        "    l, g = value_and_grad(cnn_loss)(params, x, y)\n",
        "    params = jax.tree_map(lambda p, gg: p - lr * gg, params, g)\n",
        "    return params, l\n",
        "\n",
        "params_cnn = init_cnn(random.PRNGKey(2))\n",
        "for step in range(200):\n",
        "    params_cnn, l = cnn_step(params_cnn, imgs, labels)\n",
        "print('Final CNN loss:', float(l))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12) RNN via `lax.scan` (sequence modeling)\n",
        "\n",
        "```python\n",
        "T, B, D_in, D_hid = 50, 32, 16, 32\n",
        "key = random.PRNGKey(3)\n",
        "xs = random.normal(key, (T, B, D_in))\n",
        "ys = random.normal(key, (T, B, D_hid))\n",
        "\n",
        "def init_rnn(key):\n",
        "    k1,k2,k3 = random.split(key, 3)\n",
        "    params = dict(\n",
        "        Wx = random.normal(k1, (D_in, D_hid)) / jnp.sqrt(D_in),\n",
        "        Wh = random.normal(k2, (D_hid, D_hid)) / jnp.sqrt(D_hid),\n",
        "        b  = jnp.zeros((D_hid,))\n",
        "    )\n",
        "    return params\n",
        "\n",
        "def rnn_step(params, h, x):\n",
        "    h_new = jnp.tanh(x @ params['Wx'] + h @ params['Wh'] + params['b'])\n",
        "    return h_new, h_new\n",
        "\n",
        "def rnn_apply(params, x_seq, h0):\n",
        "    hT, hs = lax.scan(lambda h, x: rnn_step(params, h, x), h0, x_seq)\n",
        "    return hT, hs\n",
        "\n",
        "params_rnn = init_rnn(key)\n",
        "h0 = jnp.zeros((B, D_hid))\n",
        "_, hs = rnn_apply(params_rnn, xs, h0)\n",
        "print('RNN hidden seq shape:', hs.shape)  # (T, B, D_hid)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Compact Transformer encoder block (no Flax)\n",
        "\n",
        "```python\n",
        "def layer_norm(x, eps=1e-5):\n",
        "    m = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    v = jnp.mean((x - m)**2, axis=-1, keepdims=True)\n",
        "    return (x - m) / jnp.sqrt(v + eps)\n",
        "\n",
        "def init_transformer_block(key, d_model=128, n_heads=4, d_ff=256):\n",
        "    kq, kk, kv, ko, k1, k2 = random.split(key, 6)\n",
        "    params = dict(\n",
        "        Wq = random.normal(kq, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wk = random.normal(kk, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wv = random.normal(kv, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        Wo = random.normal(ko, (d_model, d_model)) / jnp.sqrt(d_model),\n",
        "        W1 = random.normal(k1, (d_model, d_ff)) / jnp.sqrt(d_model),\n",
        "        W2 = random.normal(k2, (d_ff, d_model)) / jnp.sqrt(d_ff),\n",
        "        b1 = jnp.zeros((d_ff,)),\n",
        "        b2 = jnp.zeros((d_model,)),\n",
        "    )\n",
        "    head_dim = d_model // n_heads\n",
        "    return params, head_dim, n_heads\n",
        "\n",
        "def split_heads(x, n_heads):\n",
        "    B,T,D = x.shape\n",
        "    return x.reshape(B, T, n_heads, D//n_heads).transpose(0,2,1,3)  # (B,H,T,dh)\n",
        "\n",
        "def combine_heads(x):\n",
        "    B,H,T,dh = x.shape\n",
        "    return x.transpose(0,2,1,3).reshape(B, T, H*dh)\n",
        "\n",
        "def mhsa(params, x, head_dim, n_heads, mask=None):\n",
        "    Q = x @ params['Wq']; K = x @ params['Wk']; V = x @ params['Wv']\n",
        "    Qh, Kh, Vh = split_heads(Q, n_heads), split_heads(K, n_heads), split_heads(V, n_heads)\n",
        "    scale = 1.0 / jnp.sqrt(head_dim)\n",
        "    attn = jnp.einsum('bhtd,bhTd->bhtT', Qh, Kh) * scale  # (B,H,T,T)\n",
        "    if mask is not None:\n",
        "        attn = jnp.where(mask, attn, -1e9)\n",
        "    probs = softmax(attn, axis=-1)\n",
        "    out = jnp.einsum('bhtT,bhTd->bhtd', probs, Vh)\n",
        "    out = combine_heads(out) @ params['Wo']\n",
        "    return out\n",
        "\n",
        "def transformer_block(params, x, head_dim, n_heads, mask=None):\n",
        "    x = layer_norm(x + mhsa(params, x, head_dim, n_heads, mask))\n",
        "    y = jnp.tanh(x @ params['W1'] + params['b1'])\n",
        "    y = y @ params['W2'] + params['b2']\n",
        "    x = layer_norm(x + y)\n",
        "    return x\n",
        "\n",
        "# Demo\n",
        "B, T, D = 8, 32, 128\n",
        "key = random.PRNGKey(0)\n",
        "x = random.normal(key, (B, T, D))\n",
        "params_tx, head_dim, n_heads = init_transformer_block(key, d_model=D, n_heads=4, d_ff=256)\n",
        "y = transformer_block(params_tx, x, head_dim, n_heads)\n",
        "print('Transformer out:', y.shape)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 14) Optimization with Optax (adamw, schedules, clipping)\n",
        "\n",
        "```python\n",
        "try:\n",
        "    import optax\n",
        "\n",
        "    def make_optimizer(lr=1e-3, wd=1e-4):\n",
        "        return optax.chain(\n",
        "            optax.clip_by_global_norm(1.0),\n",
        "            optax.adamw(learning_rate=lr, weight_decay=wd)\n",
        "        )\n",
        "\n",
        "    @dataclass\n",
        "    class TrainState:\n",
        "        params: Any\n",
        "        opt_state: Any\n",
        "\n",
        "    def init_train_state(params, opt):\n",
        "        return TrainState(params=params, opt_state=opt.init(params))\n",
        "\n",
        "    def apply_updates(params, updates):\n",
        "        return optax.apply_updates(params, updates)\n",
        "\n",
        "    @jit\n",
        "    def train_step_optax(state, x, y, opt):\n",
        "        loss, grads = value_and_grad(binary_loss)(state.params, x, y)\n",
        "        updates, new_opt_state = opt.update(grads, state.opt_state, state.params)\n",
        "        new_params = apply_updates(state.params, updates)\n",
        "        return TrainState(new_params, new_opt_state), loss\n",
        "\n",
        "    # Demo on MLP data\n",
        "    key = random.PRNGKey(0)\n",
        "    params0 = init_mlp(key, [2, 64, 64, 1])\n",
        "    opt = make_optimizer(1e-2, 1e-4)\n",
        "    state = init_train_state(params0, opt)\n",
        "    for step in range(200):\n",
        "        state, l = train_step_optax(state, X, y, opt)\n",
        "    print('Optax demo loss:', float(l))\n",
        "\n",
        "except Exception as e:\n",
        "    print('Optax not available:', e)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 15) Memory: rematerialization (remat/checkpoint), scans vs vmaps, donation\n",
        "\n",
        "```python\n",
        "# Rematerialization: recompute activations during backward to save memory\n",
        "from jax import checkpoint as remat\n",
        "\n",
        "def big_chain(x, depth=10):\n",
        "    for _ in range(depth):\n",
        "        x = jnp.tanh(x @ x.T)\n",
        "    return x\n",
        "\n",
        "@jit\n",
        "def f_no_remat(x): return big_chain(x)\n",
        "\n",
        "@jit\n",
        "def f_with_remat(x): return remat(big_chain)(x)\n",
        "\n",
        "Xbig = random.normal(random.PRNGKey(0), (128, 128))\n",
        "_ = f_no_remat(Xbig).block_until_ready()\n",
        "_ = f_with_remat(Xbig).block_until_ready()\n",
        "print('Remat demo complete (profile on your hardware for real gains).')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 16) Saving / loading params (simple & robust)\n",
        "\n",
        "Two simple approaches:\n",
        "\n",
        "1. **Pickle** (easy & preserves structure; fine for experiments)\n",
        "2. **NumPy `.npz` with metadata** (portable arrays; stores treedef in an object field)\n",
        "\n",
        "```python\n",
        "# 1) Pickle (simple)\n",
        "def tree_save_pickle(path, pytree):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(pytree, f)\n",
        "\n",
        "def tree_load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "tree_save_pickle('params_demo.pkl', params)\n",
        "params_loaded = tree_load_pickle('params_demo.pkl')\n",
        "print('pickle load ok:', jax.tree_util.tree_structure(params) == jax.tree_util.tree_structure(params_loaded))\n",
        "\n",
        "# 2) NPZ with treedef (object array)\n",
        "from jax import tree_util\n",
        "\n",
        "def tree_save_npz(path, pytree):\n",
        "    leaves, treedef = tree_util.tree_flatten(pytree)\n",
        "    save_dict = {f'leaf_{i}': np.asarray(leaf) for i, leaf in enumerate(leaves)}\n",
        "    save_dict['treedef'] = np.array([treedef], dtype=object)\n",
        "    np.savez(path, **save_dict)\n",
        "\n",
        "def tree_load_npz(path):\n",
        "    with np.load(path, allow_pickle=True) as data:\n",
        "        treedef = data['treedef'][0]\n",
        "        leaf_keys = sorted([k for k in data.files if k.startswith('leaf_')],\n",
        "                           key=lambda s: int(s.split('_')[1]))\n",
        "        leaves = [data[k] for k in leaf_keys]\n",
        "    return tree_util.tree_unflatten(treedef, leaves)\n",
        "\n",
        "tree_save_npz('params_demo.npz', params)\n",
        "params2 = tree_load_npz('params_demo.npz')\n",
        "print('npz load ok:', jax.tree_util.tree_structure(params) == jax.tree_util.tree_structure(params2))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 17) Debugging & profiling\n",
        "\n",
        "* `jax.debug.print` works inside `jit`.\n",
        "* `JAX_DEBUG_NANS=1` + `jax_enable_x64=True` for numerics hunts (slower).\n",
        "* Benchmark with warmup + `.block_until_ready()`; prefer profilers for full runs.\n",
        "* Keep input shapes/dtypes consistent to avoid retracing.\n",
        "\n",
        "```python\n",
        "@jit\n",
        "def foo(x):\n",
        "    jax.debug.print('Inside jit, mean(x) = {}', jnp.mean(x))\n",
        "    return x + 1\n",
        "\n",
        "_ = foo(jnp.arange(4.))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 18) Interop: jax.numpy ↔ numpy, device transfers, jax2tf (note)\n",
        "\n",
        "```python\n",
        "arr = jnp.arange(5.)\n",
        "host_arr = np.array(arr)  # transfers to host, becomes NumPy array\n",
        "print(type(host_arr), host_arr)\n",
        "\n",
        "# Explicit device <-> host\n",
        "on_device = jax.device_put(host_arr)\n",
        "back_to_host = jax.device_get(on_device)\n",
        "print(type(back_to_host), back_to_host)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 19) (Optional) Quick Flax/Linen taste: a tiny MLP\n",
        "\n",
        "If you installed `flax`, here’s a minimal pattern.\n",
        "\n",
        "```python\n",
        "try:\n",
        "    import flax.linen as nn\n",
        "    import optax\n",
        "    from flax.training import train_state\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        hidden: int = 64\n",
        "        @nn.compact\n",
        "        def __call__(self, x):\n",
        "            x = nn.tanh(nn.Dense(self.hidden)(x))\n",
        "            x = nn.tanh(nn.Dense(self.hidden)(x))\n",
        "            x = nn.Dense(1)(x)\n",
        "            return x.squeeze(-1)\n",
        "\n",
        "    def loss_fn(params, model, x, y):\n",
        "        logits = model.apply({'params': params}, x)\n",
        "        return jnp.mean(jnp.logaddexp(0.0, -y * logits))\n",
        "\n",
        "    class State(train_state.TrainState): pass\n",
        "\n",
        "    model = MLP(64)\n",
        "    key = random.PRNGKey(0)\n",
        "    params_flax = model.init(key, X)['params']\n",
        "    tx = optax.adamw(1e-2)\n",
        "    state = State.create(apply_fn=model.apply, params=params_flax, tx=tx)\n",
        "\n",
        "    @jit\n",
        "    def train_step_flax(state, x, y):\n",
        "        loss, grads = value_and_grad(loss_fn)(state.params, model, x, y)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, loss\n",
        "\n",
        "    for step in range(200):\n",
        "        state, l = train_step_flax(state, X, y)\n",
        "    print('Flax demo loss:', float(l))\n",
        "except Exception as e:\n",
        "    print('Flax not available or import failed:', e)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 20) (Optional) Inspect compilation cost & caching\n",
        "\n",
        "You’ll often see “first call slow, rest fast”. This shows warmup vs. steady-state.\n",
        "\n",
        "```python\n",
        "@jit\n",
        "def heavy_matmul(a, b):\n",
        "    return a @ b + jnp.tanh(a @ b)\n",
        "\n",
        "A = random.normal(random.PRNGKey(0), (2048, 2048), dtype=jnp.float32)\n",
        "B = random.normal(random.PRNGKey(1), (2048, 2048), dtype=jnp.float32)\n",
        "\n",
        "t0 = time.time(); _ = heavy_matmul(A, B).block_until_ready(); t1 = time.time()\n",
        "t2 = time.time(); _ = heavy_matmul(A, B).block_until_ready(); t3 = time.time()\n",
        "print(f'compile+run: {(t1-t0):.3f}s, run-only: {(t3-t2):.3f}s')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 21) (Optional) Mixed precision quick pattern\n",
        "\n",
        "Keep “master” weights in fp32; cast activations/weights on hot ops if desired.\n",
        "\n",
        "```python\n",
        "def mixed_precision_mlp_apply(params, x):\n",
        "    # Example casting: compute in bfloat16 (works well on TPU; on GPUs prefer float16/TF32)\n",
        "    x16 = x.astype(jnp.bfloat16)\n",
        "    outs = []\n",
        "    for (W, b) in params[:-1]:\n",
        "        y = (x16 @ W.astype(jnp.bfloat16) + b.astype(jnp.bfloat16))\n",
        "        x16 = jnp.tanh(y)\n",
        "    W, b = params[-1]\n",
        "    logits16 = x16 @ W.astype(jnp.bfloat16) + b.astype(jnp.bfloat16)\n",
        "    return logits16.astype(jnp.float32)  # back to fp32 at boundaries\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 22) (Optional) Practical batching + `vmap` + `jit`\n",
        "\n",
        "```python\n",
        "def dataloader(X, y, batch=128, shuffle=True, seed=0):\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            rng.shuffle(idx)\n",
        "        for i in range(0, n, batch):\n",
        "            j = idx[i:i+batch]\n",
        "            yield X[j], y[j]\n",
        "\n",
        "@jit\n",
        "def step(params, xb, yb, lr=1e-2):\n",
        "    loss, grads = value_and_grad(binary_loss)(params, xb, yb)\n",
        "    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "    return params, loss\n",
        "\n",
        "gen = dataloader(X, y, batch=128, shuffle=True, seed=0)\n",
        "params_b = init_mlp(random.PRNGKey(0), [2, 64, 64, 1])\n",
        "for step_i in range(50):\n",
        "    xb, yb = next(gen)\n",
        "    params_b, l = step(params_b, xb, yb)\n",
        "print('Minibatch training loss (last):', float(l))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 23) (Optional) Common gotchas (quick checklist)\n",
        "\n",
        "* Py control flow inside `jit` that depends on data → use `lax.cond/scan/while_loop`.\n",
        "* Inconsistent shapes/dtypes across calls → retracing & recompilation.\n",
        "* PRNG keys reused accidentally → non-random behavior.\n",
        "* Mutating globals inside `jit` → doesn’t work; think functional.\n",
        "* Timing without `.block_until_ready()` → misleading.\n",
        "\n",
        "---\n",
        "\n",
        "## 24) Cheat sheet\n",
        "\n",
        "* **Core transforms**: `jit`, `grad`, `value_and_grad`, `vmap`, `pmap`, `pjit`\n",
        "* **Autodiff**: `jvp`, `vjp`, `jacfwd`, `jacrev`, `custom_vjp`, `custom_jvp`\n",
        "* **Control flow**: `lax.cond`, `lax.scan`, `lax.while_loop`\n",
        "* **Performance**: donation (`donate_argnums`), remat (`jax.checkpoint`), `.block_until_ready()`, `lower(...).as_text()`\n",
        "* **Random**: `PRNGKey`, `split`, `fold_in`\n",
        "* **Pytrees**: `tree_map`, `tree_flatten`, `tree_unflatten`\n",
        "* **Sharding**: `Mesh`, `PartitionSpec`, `NamedSharding`, `pjit`\n",
        "* **Numerics**: stable softmax/logsumexp, gradient clipping, mixed precision\n",
        "* **IO**: pickle (simple), npz (portable), Orbax (large jobs; if installed)\n",
        "\n",
        "---\n",
        "\n",
        "that’s the full scaffold. paste into a notebook cell-by-cell (top to bottom) and you’ll have a working playground. want me to extend any part (e.g., full pjit example across 2+ GPUs, dataset loaders, LR schedulers, gradient checkpointed Transformer stack, Orbax checkpointing, or TPU tips)?\n"
      ],
      "metadata": {
        "id": "E3fTmte2TXw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweet — here’s a clean, from-scratch **JAX CNN on MNIST** you can paste into your notebook. it loads MNIST (tfds first; falls back to Keras/Torch if needed), builds a small ConvNet with pure JAX/LAX, and trains/evaluates it. everything’s commented so you can tweak fast.\n",
        "\n",
        "---\n",
        "\n",
        "# CNN on MNIST (pure JAX/LAX, from scratch)\n",
        "\n",
        "### 0) Installs (as needed) + imports\n",
        "\n",
        "> If you don’t have any of these, uncomment the appropriate `%pip` lines and run once.\n",
        "\n",
        "```python\n",
        "# --- optional installs (pick one data source you prefer) ---\n",
        "# %pip install -U tensorflow-datasets\n",
        "# %pip install -U tensorflow-cpu          # only if you plan to use keras.datasets.mnist\n",
        "# %pip install -U torch torchvision       # only if you plan to use torchvision\n",
        "\n",
        "# --- JAX/plotting (if you skipped them earlier) ---\n",
        "# %pip install -U \"jax[cpu]\"              # or the right cuda wheel for your GPU\n",
        "# %pip install -U matplotlib optax        # optax optional (we fallback to SGD)\n",
        "\n",
        "import time, math, pickle, os\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Tuple, Dict, Iterable, Optional\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "from jax import jit, value_and_grad, random, lax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"JAX:\", jax.__version__, \"backend:\", jax.default_backend(), \"devices:\", jax.devices())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1) MNIST loader (tfds → keras → torchvision fallback)\n",
        "\n",
        "```python\n",
        "def load_mnist() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Returns (x_train, y_train, x_test, y_test) with shapes:\n",
        "       - x_*: (N, 28, 28, 1) float32 in [0, 1]\n",
        "       - y_*: (N,) int64 labels 0..9\n",
        "    Tries tensorflow_datasets first, then keras, then torchvision.\n",
        "    \"\"\"\n",
        "    # 1) tensorflow_datasets\n",
        "    try:\n",
        "        import tensorflow_datasets as tfds\n",
        "        (x_train, y_train) = tfds.as_numpy(\n",
        "            tfds.load(\"mnist\", split=\"train\", batch_size=-1, as_supervised=True)\n",
        "        )\n",
        "        (x_test, y_test) = tfds.as_numpy(\n",
        "            tfds.load(\"mnist\", split=\"test\", batch_size=-1, as_supervised=True)\n",
        "        )\n",
        "        x_train = x_train.astype(np.float32) / 255.0\n",
        "        x_test  = x_test.astype(np.float32) / 255.0\n",
        "        if x_train.ndim == 3:\n",
        "            x_train = x_train[..., None]\n",
        "            x_test  = x_test[..., None]\n",
        "        return x_train, y_train.astype(np.int64), x_test, y_test.astype(np.int64)\n",
        "    except Exception as e:\n",
        "        print(\"[load_mnist] tfds path failed:\", e)\n",
        "\n",
        "    # 2) keras.datasets.mnist\n",
        "    try:\n",
        "        from tensorflow.keras.datasets import mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = (x_train.astype(np.float32) / 255.0)[..., None]\n",
        "        x_test  = (x_test.astype(np.float32) / 255.0)[..., None]\n",
        "        return x_train, y_train.astype(np.int64), x_test, y_test.astype(np.int64)\n",
        "    except Exception as e:\n",
        "        print(\"[load_mnist] keras path failed:\", e)\n",
        "\n",
        "    # 3) torchvision\n",
        "    try:\n",
        "        from torchvision import datasets, transforms\n",
        "        import torch\n",
        "        tfm = transforms.Compose([transforms.ToTensor()])  # returns (C,H,W) in [0,1]\n",
        "        train_ds = datasets.MNIST(root=\"./data\", train=True, transform=tfm, download=True)\n",
        "        test_ds  = datasets.MNIST(root=\"./data\", train=False, transform=tfm, download=True)\n",
        "        def _to_numpy(ds):\n",
        "            xs = []\n",
        "            ys = []\n",
        "            for img, y in ds:\n",
        "                arr = img.numpy().transpose(1,2,0)  # (H,W,C)\n",
        "                xs.append(arr)\n",
        "                ys.append(int(y))\n",
        "            return np.stack(xs, 0).astype(np.float32), np.array(ys, dtype=np.int64)\n",
        "        x_train, y_train = _to_numpy(train_ds)\n",
        "        x_test, y_test   = _to_numpy(test_ds)\n",
        "        return x_train, y_train, x_test, y_test\n",
        "    except Exception as e:\n",
        "        print(\"[load_mnist] torchvision path failed:\", e)\n",
        "        raise RuntimeError(\"Could not load MNIST with tfds/keras/torchvision. Install one of them and retry.\")\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()\n",
        "print(\"Train:\", x_train.shape, y_train.shape, \"| Test:\", x_test.shape, y_test.shape)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Dataloader (numpy → mini-batches)\n",
        "\n",
        "```python\n",
        "def dataloader(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool=True, seed: int=0) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            rng.shuffle(idx)\n",
        "        for i in range(0, n, batch_size):\n",
        "            j = idx[i:i+batch_size]\n",
        "            yield X[j], y[j]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Model: ConvNet with max-pooling → dense → logits (10 classes)\n",
        "\n",
        "* Conv(3×3, 32) → ReLU → MaxPool(2×2, s=2)\n",
        "* Conv(3×3, 64) → ReLU → MaxPool(2×2, s=2)\n",
        "* Flatten → Dense(128) → ReLU → Dense(10)\n",
        "\n",
        "Weights are He-initialized (good for ReLU).\n",
        "\n",
        "```python\n",
        "def he_init_std(fan_in: int) -> float:\n",
        "    return math.sqrt(2.0 / fan_in)\n",
        "\n",
        "def init_cnn_params(key: jax.Array):\n",
        "    k1, k2, k3, k4 = random.split(key, 4)\n",
        "    # Conv kernels: (KH, KW, Cin, Cout)\n",
        "    W1 = random.normal(k1, (3, 3, 1, 32)) * he_init_std(3*3*1)\n",
        "    b1 = jnp.zeros((32,))\n",
        "    W2 = random.normal(k2, (3, 3, 32, 64)) * he_init_std(3*3*32)\n",
        "    b2 = jnp.zeros((64,))\n",
        "    # After two 2x2 pools: 28 -> 14 -> 7, channels = 64\n",
        "    flat_dim = 7*7*64\n",
        "    W3 = random.normal(k3, (flat_dim, 128)) * he_init_std(flat_dim)\n",
        "    b3 = jnp.zeros((128,))\n",
        "    W4 = random.normal(k4, (128, 10)) * (1.0 / math.sqrt(128.0))\n",
        "    b4 = jnp.zeros((10,))\n",
        "    return (W1,b1,W2,b2,W3,b3,W4,b4)\n",
        "\n",
        "def max_pool_2x2(x: jax.Array) -> jax.Array:\n",
        "    # x: (N,H,W,C)\n",
        "    return lax.reduce_window(\n",
        "        x,\n",
        "        -jnp.inf,\n",
        "        lax.max,\n",
        "        window_dimensions=(1,2,2,1),\n",
        "        window_strides=(1,2,2,1),\n",
        "        padding=\"VALID\"\n",
        "    )\n",
        "\n",
        "def cnn_apply(params, x):\n",
        "    W1,b1,W2,b2,W3,b3,W4,b4 = params\n",
        "    # Conv1 + ReLU + Pool\n",
        "    y = lax.conv_general_dilated(x, W1, window_strides=(1,1), padding=\"SAME\")\n",
        "    y = jnn.relu(y + b1)\n",
        "    y = max_pool_2x2(y)\n",
        "    # Conv2 + ReLU + Pool\n",
        "    y = lax.conv_general_dilated(y, W2, window_strides=(1,1), padding=\"SAME\")\n",
        "    y = jnn.relu(y + b2)\n",
        "    y = max_pool_2x2(y)\n",
        "    # Flatten\n",
        "    y = y.reshape((y.shape[0], -1))\n",
        "    # Dense -> ReLU -> Dense\n",
        "    y = jnn.relu(y @ W3 + b3)\n",
        "    logits = y @ W4 + b4\n",
        "    return logits\n",
        "\n",
        "def loss_and_metrics(params, xb, yb):\n",
        "    logits = cnn_apply(params, xb)\n",
        "    logp = jnn.log_softmax(logits, axis=-1)\n",
        "    nll = -jnp.take_along_axis(logp, yb[:, None], axis=1).mean()\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    acc = (preds == yb).mean()\n",
        "    return nll, acc\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Optimizer (Optax AdamW if available; otherwise plain SGD)\n",
        "\n",
        "```python\n",
        "try:\n",
        "    import optax\n",
        "    use_optax = True\n",
        "    opt = optax.adamw(learning_rate=1e-3, weight_decay=1e-4)\n",
        "    @dataclass\n",
        "    class TrainState:\n",
        "        params: Any\n",
        "        opt_state: Any\n",
        "    def init_state(params):\n",
        "        return TrainState(params=params, opt_state=opt.init(params))\n",
        "    @jit\n",
        "    def train_step(state: \"TrainState\", xb: jax.Array, yb: jax.Array):\n",
        "        (loss, acc), grads = value_and_grad(loss_and_metrics, has_aux=True)(state.params, xb, yb)\n",
        "        updates, new_opt_state = opt.update(grads, state.opt_state, state.params)\n",
        "        new_params = optax.apply_updates(state.params, updates)\n",
        "        return TrainState(new_params, new_opt_state), loss, acc\n",
        "except Exception as e:\n",
        "    print(\"[optimizer] Optax not available, falling back to SGD. Reason:\", e)\n",
        "    use_optax = False\n",
        "    lr = 1e-2\n",
        "    @dataclass\n",
        "    class TrainState:\n",
        "        params: Any\n",
        "    def init_state(params):\n",
        "        return TrainState(params=params)\n",
        "    @jit\n",
        "    def train_step(state: \"TrainState\", xb: jax.Array, yb: jax.Array):\n",
        "        (loss, acc), grads = value_and_grad(loss_and_metrics, has_aux=True)(state.params, xb, yb)\n",
        "        new_params = jax.tree_map(lambda p, g: p - lr * g, state.params, grads)\n",
        "        return TrainState(new_params), loss, acc\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Evaluation helpers\n",
        "\n",
        "```python\n",
        "@jit\n",
        "def eval_batch(params, xb, yb):\n",
        "    return loss_and_metrics(params, xb, yb)\n",
        "\n",
        "def evaluate(params, X, y, batch_size=1024):\n",
        "    n = X.shape[0]\n",
        "    losses = []\n",
        "    accs = []\n",
        "    for i in range(0, n, batch_size):\n",
        "        xb = jnp.array(X[i:i+batch_size])\n",
        "        yb = jnp.array(y[i:i+batch_size])\n",
        "        l, a = eval_batch(params, xb, yb)\n",
        "        losses.append(float(l))\n",
        "        accs.append(float(a))\n",
        "    return np.mean(losses), np.mean(accs)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Train loop\n",
        "\n",
        "```python\n",
        "# Hyperparams\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "seed = 0\n",
        "\n",
        "# Initialize params\n",
        "key = random.PRNGKey(seed)\n",
        "params = init_cnn_params(key)\n",
        "state = init_state(params)\n",
        "\n",
        "# Build dataloader\n",
        "train_iter = dataloader(x_train, y_train, batch_size=batch_size, shuffle=True, seed=seed)\n",
        "\n",
        "# Training\n",
        "steps_per_epoch = math.ceil(x_train.shape[0] / batch_size)\n",
        "hist = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_losses, epoch_accs = [], []\n",
        "    for _ in range(steps_per_epoch):\n",
        "        xb_np, yb_np = next(train_iter)\n",
        "        xb = jnp.array(xb_np)  # moved to device automatically\n",
        "        yb = jnp.array(yb_np)\n",
        "        state, loss, acc = train_step(state, xb, yb)\n",
        "        epoch_losses.append(float(loss))\n",
        "        epoch_accs.append(float(acc))\n",
        "    tr_loss, tr_acc = np.mean(epoch_losses), np.mean(epoch_accs)\n",
        "    te_loss, te_acc = evaluate(state.params if hasattr(state, \"params\") else state, x_test, y_test)\n",
        "    hist[\"train_loss\"].append(tr_loss)\n",
        "    hist[\"train_acc\"].append(tr_acc)\n",
        "    hist[\"test_loss\"].append(te_loss)\n",
        "    hist[\"test_acc\"].append(te_acc)\n",
        "    print(f\"[epoch {epoch:02d}] train loss {tr_loss:.4f} acc {tr_acc*100:.2f}% | \"\n",
        "          f\"test loss {te_loss:.4f} acc {te_acc*100:.2f}%\")\n",
        "t1 = time.time()\n",
        "print(f\"Total time: {t1 - t0:.2f}s\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7) Quick curves + sample predictions (optional)\n",
        "\n",
        "```python\n",
        "plt.figure(); plt.plot(hist[\"train_loss\"], label=\"train\"); plt.plot(hist[\"test_loss\"], label=\"test\"); plt.title(\"Loss\"); plt.legend(); plt.show()\n",
        "plt.figure(); plt.plot(hist[\"train_acc\"], label=\"train\"); plt.plot(hist[\"test_acc\"], label=\"test\"); plt.title(\"Accuracy\"); plt.legend(); plt.show()\n",
        "\n",
        "# Show a few predictions\n",
        "def predict(params, x):\n",
        "    logits = cnn_apply(params, x)\n",
        "    return jnp.argmax(logits, axis=-1)\n",
        "\n",
        "idx = np.random.default_rng(0).integers(0, x_test.shape[0], size=6)\n",
        "imgs = jnp.array(x_test[idx])\n",
        "preds = predict(state.params if hasattr(state, \"params\") else state, imgs)\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "for i, k in enumerate(idx):\n",
        "    plt.subplot(1,6,i+1); plt.imshow(x_test[k, ..., 0], cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"y={y_test[k]}\\nŷ={int(preds[i])}\")\n",
        "plt.tight_layout(); plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Notes & tips\n",
        "\n",
        "* This should reach \\~99% test acc in a handful of epochs on CPU/GPU (exact numbers vary).\n",
        "* For faster convergence, bump epochs to 10–15, switch to AdamW (Optax path), or add a learning-rate schedule.\n",
        "* To go bigger: add dropout, batch norm, or a third conv block; swap ReLU→GELU; try data augmentation (random shifts/crops).\n",
        "* If you hit GPU OOM, reduce batch size.\n",
        "* If you want a Flax/Linen version (module definitions, state handling, jit/pjit-ready), say the word and I’ll drop it in.\n"
      ],
      "metadata": {
        "id": "Gd0qHVx4Uev_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}